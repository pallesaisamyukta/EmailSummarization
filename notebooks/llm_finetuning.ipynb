{"cells":[{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T03:52:02.769078Z","iopub.status.busy":"2024-03-20T03:52:02.768093Z","iopub.status.idle":"2024-03-20T03:52:18.904129Z","shell.execute_reply":"2024-03-20T03:52:18.902984Z","shell.execute_reply.started":"2024-03-20T03:52:02.769045Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.38.1)\n","Requirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.1.0)\n","Requirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (2.1.2)\n","Collecting rouge-score\n","  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25ldone\n","\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.20.3)\n","Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\n","Requirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\n","Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.31.0)\n","Requirement already satisfied: tokenizers<0.19,>=0.14 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.15.2)\n","Requirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.2)\n","Requirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.1)\n","Requirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (11.0.0)\n","Requirement already satisfied: dill in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\n","Requirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.1.4)\n","Requirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\n","Requirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\n","Requirement already satisfied: fsspec>=2021.05.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]>=2021.05.0->datasets) (2024.2.0)\n","Requirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\n","Requirement already satisfied: responses<0.19 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.18.0)\n","Requirement already satisfied: typing-extensions in /opt/conda/lib/python3.10/site-packages (from torch) (4.9.0)\n","Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch) (1.12)\n","Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch) (3.2.1)\n","Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch) (3.1.2)\n","Requirement already satisfied: absl-py in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.4.0)\n","Requirement already satisfied: nltk in /opt/conda/lib/python3.10/site-packages (from rouge-score) (3.2.4)\n","Requirement already satisfied: six>=1.14.0 in /opt/conda/lib/python3.10/site-packages (from rouge-score) (1.16.0)\n","Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\n","Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\n","Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\n","Requirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\n","Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\n","Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.2.2)\n","Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch) (2.1.3)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\n","Requirement already satisfied: tzdata>=2022.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\n","Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch) (1.3.0)\n","Building wheels for collected packages: rouge-score\n","  Building wheel for rouge-score (setup.py) ... \u001b[?25ldone\n","\u001b[?25h  Created wheel for rouge-score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=8ed5c30dd4f6419c93401fad12226f53b7325f6deaf0567785a1eb4bac39a0a2\n","  Stored in directory: /root/.cache/pip/wheels/5f/dd/89/461065a73be61a532ff8599a28e9beef17985c9e9c31e541b4\n","Successfully built rouge-score\n","Installing collected packages: rouge-score\n","Successfully installed rouge-score-0.1.2\n","Note: you may need to restart the kernel to use updated packages.\n"]}],"source":["%pip install transformers datasets torch rouge-score"]},{"cell_type":"code","execution_count":4,"metadata":{"execution":{"iopub.execute_input":"2024-03-20T03:55:09.159733Z","iopub.status.busy":"2024-03-20T03:55:09.159080Z","iopub.status.idle":"2024-03-20T06:35:16.292061Z","shell.execute_reply":"2024-03-20T06:35:16.291245Z","shell.execute_reply.started":"2024-03-20T03:55:09.159703Z"},"trusted":true},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"3b250d4583084a9781da944cec0aa27e","version_major":2,"version_minor":0},"text/plain":["vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"e99741f7657b4c5e9cd877b4ccd3ee42","version_major":2,"version_minor":0},"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0374086f78a14810afe9ebd80885c424","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"5dbfd189a5d74dc79b0b46a0090602c5","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/1.72k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"10f9adec678b46879f28911d463a7311","version_major":2,"version_minor":0},"text/plain":["model.safetensors:   0%|          | 0.00/558M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Evaluating before fine-tuning...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a6946e3e6d7a4b70ab81d6e1ea67cf2e","version_major":2,"version_minor":0},"text/plain":["Downloading builder script:   0%|          | 0.00/2.16k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:3892: UserWarning: `as_target_tokenizer` is deprecated and will be removed in v5 of Transformers. You can tokenize your labels by using the argument `text_target` of the regular `__call__` method (either in the same call as your input texts if you use the same keyword arguments, or in a separate call.\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["{'rouge1': 20.1922447996289, 'rouge2': 6.297088181855775, 'rougeL': 13.43923420820447, 'rougeLsum': 13.452363592347027}\n","Epoch 1/5\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"6bf894053caf4340adca06f96f855884","version_major":2,"version_minor":0},"text/plain":["Epoch 1/5:   0%|          | 0/2440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 2/5\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6542ad9fa4c4836b6e4675a622ebe89","version_major":2,"version_minor":0},"text/plain":["Epoch 2/5:   0%|          | 0/2440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 3/5\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"33ac7784f97b4ff89fbb3a0430f37bd8","version_major":2,"version_minor":0},"text/plain":["Epoch 3/5:   0%|          | 0/2440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 4/5\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c2ac7e4ec50140d4858f2331930ea5df","version_major":2,"version_minor":0},"text/plain":["Epoch 4/5:   0%|          | 0/2440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Epoch 5/5\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"87ee875ee8334aecb00c517829b04dde","version_major":2,"version_minor":0},"text/plain":["Epoch 5/5:   0%|          | 0/2440 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stdout","output_type":"stream","text":["Evaluating after fine-tuning...\n","{'rouge1': 44.83073283674907, 'rouge2': 24.159861008906237, 'rougeL': 32.51058352181336, 'rougeLsum': 32.52312024917855}\n","Model evaluation and fine-tuning complete.\n"]}],"source":["import torch\n","import pandas as pd\n","from transformers import BartTokenizer, BartForConditionalGeneration, AdamW, get_linear_schedule_with_warmup\n","from torch.utils.data import Dataset, DataLoader\n","from tqdm.auto import tqdm\n","from datasets import load_metric\n","from sklearn.model_selection import train_test_split\n","import torch.nn as nn\n","\n","# Load dataset from CSV\n","df = pd.read_csv('/kaggle/input/ft-email-dataset/merged_email_data.csv')\n","\n","# Custom dataset class\n","class EmailDataset(Dataset):\n","    '''\n","    Custom dataset class for email summarization task\n","\n","    Args:\n","        tokenizer: PreTrainedTokenizer for tokenizing the text data\n","        data: DataFrame containing the text data\n","        max_length: Maximum length of the input sequence\n","        summary_length: Length of the target summary sequence\n","\n","    Returns:\n","        Dictionary containing the tokenized input sequence and target summary sequence\n","    '''\n","    def __init__(self, tokenizer, data, max_length=512, summary_length=128):\n","        '''\n","        Initialize the dataset class\n","        \n","        Args:\n","            tokenizer: PreTrainedTokenizer for tokenizing the text data\n","            data: DataFrame containing the text data\n","            max_length: Maximum length of the input sequence\n","            summary_length: Length of the target summary sequence\n","            \n","        Returns:\n","            None\n","        '''\n","        self.tokenizer = tokenizer\n","        self.data = data\n","        self.max_length = max_length\n","        self.summary_length = summary_length\n","\n","    def __len__(self):\n","        '''\n","        Return the length of the dataset\n","        \n","        Args:\n","            None\n","            \n","        Returns:\n","            Length of the dataset\n","        '''\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        '''\n","        Return the tokenized input sequence and target summary sequence\n","        \n","        Args:\n","            idx: Index of the dataset\n","            \n","        Returns:\n","            Dictionary containing the tokenized input sequence and target summary sequence\n","        '''\n","        item = self.data.iloc[idx]\n","        thread = item['body']  # Assuming 'body' contains the email threads\n","        summary = item['summary']\n","        \n","        model_input = self.tokenizer(thread, max_length=self.max_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n","        with self.tokenizer.as_target_tokenizer():\n","            labels = self.tokenizer(summary, max_length=self.summary_length, truncation=True, padding='max_length', return_tensors=\"pt\")\n","        \n","        model_input[\"labels\"] = labels[\"input_ids\"].squeeze()\n","        \n","        return {key: val.squeeze() for key, val in model_input.items()}\n","\n","# Initialize tokenizer\n","tokenizer = BartTokenizer.from_pretrained('facebook/bart-base')\n","\n","# Split dataset into train and validation sets\n","train_df, val_df = train_test_split(df, test_size=0.1, random_state=42)\n","\n","# Prepare train and validation datasets\n","train_dataset = EmailDataset(tokenizer, train_df)\n","val_dataset = EmailDataset(tokenizer, val_df)\n","\n","# Initialize DataLoader for train and validation sets\n","train_dataloader = DataLoader(train_dataset, batch_size=8, shuffle=True)  # Increased batch size for faster training\n","val_dataloader = DataLoader(val_dataset, batch_size=8)  # Increased batch size for faster evaluation\n","\n","# Initialize BART model\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","bart_model = BartForConditionalGeneration.from_pretrained('facebook/bart-base')\n","bart_model.to(device)\n","\n","# Evaluation function with ROUGE scores\n","def evaluate(model, dataloader):\n","    '''\n","    Evaluate the model on the validation set using ROUGE scores\n","    \n","    Args:\n","        model: PreTrainedModel for evaluation\n","        dataloader: DataLoader for the validation set\n","        \n","    Returns:\n","        Dictionary containing ROUGE scores\n","    '''\n","    rouge = load_metric(\"rouge\")\n","    model.eval()\n","    for batch in dataloader:\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        with torch.no_grad():\n","            generated_ids = model.generate(input_ids, attention_mask=attention_mask, max_length=128, num_beams=4, early_stopping=True)\n","            preds = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in generated_ids]\n","            references = [tokenizer.decode(g, skip_special_tokens=True, clean_up_tokenization_spaces=False) for g in labels]\n","            \n","            rouge.add_batch(predictions=preds, references=references)\n","\n","    result = rouge.compute()\n","    return {key: value.mid.fmeasure * 100 for key, value in result.items()}\n","\n","# Evaluate before fine-tuning\n","print(\"Evaluating before fine-tuning...\")\n","before_scores = evaluate(bart_model, val_dataloader)\n","print(before_scores)\n","\n","# Prepare optimizer and scheduler\n","optimizer = AdamW(bart_model.parameters(), lr=5e-5)\n","epochs = 5\n","total_steps = len(train_dataloader) * epochs\n","scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)\n","\n","# Fine-tuning\n","for epoch in range(epochs):\n","    print(f\"Epoch {epoch + 1}/{epochs}\")\n","    bart_model.train()\n","    for batch in tqdm(train_dataloader, desc=f\"Epoch {epoch + 1}/{epochs}\"):\n","        optimizer.zero_grad()\n","        input_ids = batch['input_ids'].to(device)\n","        attention_mask = batch['attention_mask'].to(device)\n","        labels = batch['labels'].to(device)\n","\n","        outputs = bart_model(input_ids, attention_mask=attention_mask, labels=labels)\n","        loss = outputs.loss\n","        loss.backward()\n","        optimizer.step()\n","        scheduler.step()\n","\n","# Evaluate after fine-tuning\n","print(\"Evaluating after fine-tuning...\")\n","after_scores = evaluate(bart_model, val_dataloader)\n","print(after_scores)\n","\n","print(\"Model evaluation and fine-tuning complete.\")\n","\n","# Save the fine-tuned model\n","torch.save(bart_model.state_dict(), '/kaggle/working/fine-tuned_bart_model.pt')"]}],"metadata":{"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"datasetId":4632740,"sourceId":7890792,"sourceType":"datasetVersion"}],"dockerImageVersionId":30665,"isGpuEnabled":true,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.6"}},"nbformat":4,"nbformat_minor":4}
